{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np0plMPXRvoq"
   },
   "source": [
    "# Lesson 8 - Knowledge Graph Construction - Part I\n",
    "\n",
    "With all the plans in place, it's time to construct the knowledge graph. \n",
    "\n",
    "For the **domain graph** construction, no agent is required. The construction plan has all the information needed to drive a rule-based import.\n",
    "\n",
    "<img src=\"images/domain.png\" width=\"600\">\n",
    "\n",
    "**Note**: This notebook uses Cypher queries to build the domain graph from CSV files. Don't worry if you're unfamiliar with Cypher — focus on understanding the big picture of how the structured data is transformed into a graph structure based on the construction plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single tool which will build a knowledge graph using the defined construction rules.\n",
    "- Input: `approved_construction_plan`\n",
    "- Output: a domain graph in Neo4j\n",
    "- Tools: `construct_domain_graph` + helper functions\n",
    "\n",
    "**Workflow**\n",
    "\n",
    "1. The context is initialized with an `approved_construction_plan` and `approved_files`\n",
    "2. Process all the node construction rules\n",
    "3. Process all the relationship construction rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The usual import of needed libraries, loading of environment variables, and connection to Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 301,
    "id": "sbwxKypOSBkN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from google.adk.models.lite_llm import LiteLlm # For OpenAI support\n",
    "\n",
    "# Convenience libraries for working with Neo4j inside of Google ADK\n",
    "from neo4j_for_adk import graphdb, tool_success, tool_error\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 184,
    "id": "MI_qvZJrSJuR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-CFTBNu1XqkBfiyF4PcKOlDBwcJX3X', created=1757803133, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_5d58a6052a', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Yes, I'm ready! How can I assist you today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=13, prompt_tokens=11, total_tokens=24, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier=None)\n",
      "\n",
      "OpenAI ready.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT_4O)\n",
    "\n",
    "# Test LLM with a direct call\n",
    "print(llm.llm_client.completion(model=llm.model, messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}], tools=[]))\n",
    "\n",
    "print(\"\\nOpenAI ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 116
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'success', 'query_result': [{'message': 'Neo4j is Ready!'}]}\n"
     ]
    }
   ],
   "source": [
    "# Check connection to Neo4j by sending a query\n",
    "\n",
    "neo4j_is_ready = graphdb.send_query(\"RETURN 'Neo4j is Ready!' as message\")\n",
    "\n",
    "print(neo4j_is_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo4j Components and Plugins:\n",
      "==================================================\n",
      "Error checking plugins: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `dbms.procedures` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}\n",
      "\n",
      "==================================================\n",
      "APOC Plugin Check:\n",
      "==================================================\n",
      "✅ APOC is installed - Version: 2025.08.0\n",
      "✅ APOC text functions are available (needed for entity resolution)\n"
     ]
    }
   ],
   "source": [
    "# Check what Neo4j plugins are installed\n",
    "\n",
    "plugins_query = \"\"\"\n",
    "CALL dbms.components() YIELD name, versions, edition\n",
    "UNWIND versions AS version\n",
    "RETURN name, version, edition\n",
    "UNION ALL\n",
    "CALL dbms.procedures() YIELD name\n",
    "WHERE name STARTS WITH 'apoc.'\n",
    "RETURN DISTINCT 'APOC' AS name, 'installed' AS version, 'plugin' AS edition\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    plugins_result = graphdb.send_query(plugins_query)\n",
    "    print(\"Neo4j Components and Plugins:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if plugins_result['status'] == 'success':\n",
    "        for item in plugins_result['query_result']:\n",
    "            print(f\"• {item['name']}: {item['version']} ({item['edition']})\")\n",
    "    else:\n",
    "        print(\"Error checking plugins:\", plugins_result.get('error_message', 'Unknown error'))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking plugins: {e}\")\n",
    "    \n",
    "# Alternative simple check for APOC specifically\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"APOC Plugin Check:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    apoc_check = graphdb.send_query(\"RETURN apoc.version() AS apoc_version\")\n",
    "    if apoc_check['status'] == 'success' and apoc_check['query_result']:\n",
    "        apoc_version = apoc_check['query_result'][0]['apoc_version']\n",
    "        print(f\"✅ APOC is installed - Version: {apoc_version}\")\n",
    "    else:\n",
    "        print(\"❌ APOC is not installed or not accessible\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ APOC is not installed or not accessible: {e}\")\n",
    "\n",
    "# Check if APOC text functions are available (needed for entity resolution)\n",
    "try:\n",
    "    text_func_check = graphdb.send_query(\"RETURN apoc.text.jaroWinklerDistance('test', 'test') AS similarity\")\n",
    "    if text_func_check['status'] == 'success':\n",
    "        print(\"✅ APOC text functions are available (needed for entity resolution)\")\n",
    "    else:\n",
    "        print(\"❌ APOC text functions are not available\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ APOC text functions are not available: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Tool Definitions (Domain Graph Construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `construct_domain_graph` tool is responsible for constructing the \"domain graph\" from CSV files,\n",
    "according to the approved construction plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: create_uniqueness_constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This function creates a uniqueness constraint in Neo4j to prevent duplicate nodes with the same label and property value from being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 439
   },
   "outputs": [],
   "source": [
    "def create_uniqueness_constraint(\n",
    "    label: str,\n",
    "    unique_property_key: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Creates a uniqueness constraint for a node label and property key.\n",
    "    A uniqueness constraint ensures that no two nodes with the same label and property key have the same value.\n",
    "    This improves the performance and integrity of data import and later queries.\n",
    "\n",
    "    Args:\n",
    "        label: The label of the node to create a constraint for.\n",
    "        unique_property_key: The property key that should have a unique value.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with a status key ('success' or 'error').\n",
    "        On error, includes an 'error_message' key.\n",
    "    \"\"\"    \n",
    "    # Use string formatting since Neo4j doesn't support parameterization of labels and property keys when creating a constraint\n",
    "    constraint_name = f\"{label}_{unique_property_key}_constraint\"\n",
    "    query = f\"\"\"CREATE CONSTRAINT `{constraint_name}` IF NOT EXISTS\n",
    "    FOR (n:`{label}`)\n",
    "    REQUIRE n.`{unique_property_key}` IS UNIQUE\"\"\"\n",
    "    results = graphdb.send_query(query)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: load_nodes_from_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs batch loading of nodes from a CSV file into Neo4j. It uses the `LOAD CSV` command with the `MERGE` operation to create nodes while avoiding duplicates based on the unique column. The Cypher query processes data in batches of 1000 rows for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The csv files are stored in the `/import` directory of `neo4j` database. When you use the query `LOAD CSV from \"file:///\" + $source_file`, neo4j checks the `/import` directory by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 439
   },
   "outputs": [],
   "source": [
    "def load_nodes_from_csv(\n",
    "    source_file: str,\n",
    "    label: str,\n",
    "    unique_column_name: str,\n",
    "    properties: list[str],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Batch loading of nodes from a CSV file\"\"\"\n",
    "\n",
    "    # load nodes from CSV file by merging on the unique_column_name value\n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL (row) {{\n",
    "        MERGE (n:$($label) {{ {unique_column_name} : row[$unique_column_name] }})\n",
    "        FOREACH (k IN $properties | SET n[k] = row[k])\n",
    "    }} IN TRANSACTIONS OF 1000 ROWS\n",
    "    \"\"\"\n",
    "\n",
    "    results = graphdb.send_query(query, {\n",
    "        \"source_file\": source_file,\n",
    "        \"label\": label,\n",
    "        \"unique_column_name\": unique_column_name,\n",
    "        \"properties\": properties\n",
    "    })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Domain Graph Construction\n",
    "\n",
    "This cell executes the main construction function using the approved construction plan. It builds the complete knowledge graph by importing all nodes and relationships according to the defined rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: import_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function orchestrates the node import process by first creating a uniqueness constraint and then loading nodes from the CSV file. It ensures data integrity by establishing constraints before importing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 369
   },
   "outputs": [],
   "source": [
    "def import_nodes(node_construction: dict) -> dict:\n",
    "    \"\"\"Import nodes as defined by a node construction rule.\"\"\"\n",
    "\n",
    "    # create a uniqueness constraint for the unique_column\n",
    "    uniqueness_result = create_uniqueness_constraint(\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"]\n",
    "    )\n",
    "\n",
    "    if (uniqueness_result[\"status\"] == \"error\"):\n",
    "        return uniqueness_result\n",
    "\n",
    "    # import nodes from csv\n",
    "    load_nodes_result = load_nodes_from_csv(\n",
    "        node_construction[\"source_file\"],\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"],\n",
    "        node_construction[\"properties\"]\n",
    "    )\n",
    "\n",
    "    return load_nodes_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: import_relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function imports relationships between nodes from a CSV file. It uses a Cypher query that matches existing nodes and creates relationships between them. The query finds pairs of nodes and creates relationships with specified properties between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 456
   },
   "outputs": [],
   "source": [
    "def import_relationships(relationship_construction: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Import relationships as defined by a relationship construction rule.\"\"\"\n",
    "\n",
    "    # load nodes from CSV file by merging on the unique_column_name value \n",
    "    from_node_column = relationship_construction[\"from_node_column\"]\n",
    "    to_node_column = relationship_construction[\"to_node_column\"]\n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL (row) {{\n",
    "        MATCH (from_node:$($from_node_label) {{ {from_node_column} : row[$from_node_column] }}),\n",
    "              (to_node:$($to_node_label) {{ {to_node_column} : row[$to_node_column] }} )\n",
    "        MERGE (from_node)-[r:$($relationship_type)]->(to_node)\n",
    "        FOREACH (k IN $properties | SET r[k] = row[k])\n",
    "    }} IN TRANSACTIONS OF 1000 ROWS\n",
    "    \"\"\"\n",
    "    \n",
    "    results = graphdb.send_query(query, {\n",
    "        \"source_file\": relationship_construction[\"source_file\"],\n",
    "        \"from_node_label\": relationship_construction[\"from_node_label\"],\n",
    "        \"from_node_column\": relationship_construction[\"from_node_column\"],\n",
    "        \"to_node_label\": relationship_construction[\"to_node_label\"],\n",
    "        \"to_node_column\": relationship_construction[\"to_node_column\"],\n",
    "        \"relationship_type\": relationship_construction[\"relationship_type\"],\n",
    "        \"properties\": relationship_construction[\"properties\"]\n",
    "    })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: construct_domain_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main orchestration function that builds the entire domain graph. It processes the construction plan in two phases:\n",
    "1. **Node Construction**: First imports all nodes to ensure they exist before creating relationships\n",
    "2. **Relationship Construction**: Then creates relationships between the existing nodes\n",
    "\n",
    "This two-phase approach prevents relationship creation failures due to missing nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 218
   },
   "outputs": [],
   "source": [
    "def construct_domain_graph(construction_plan: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Construct a domain graph according to a construction plan.\"\"\"\n",
    "    # first, import nodes\n",
    "    node_constructions = [value for value in construction_plan.values() if value['construction_type'] == 'node']\n",
    "    for node_construction in node_constructions:\n",
    "        import_nodes(node_construction)\n",
    "\n",
    "    # second, import relationships\n",
    "    relationship_constructions = [value for value in construction_plan.values() if value['construction_type'] == 'relationship']\n",
    "    for relationship_construction in relationship_constructions:\n",
    "        import_relationships(relationship_construction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Run construct_domain_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the approved construction plan as a dictionary containing rules for creating nodes and relationships. The plan includes:\n",
    "\n",
    "- **Node Rules**: Define how to create Assembly, Part, Product, and Supplier nodes from CSV files\n",
    "- **Relationship Rules**: Define how to create Contains, Is_Part_Of, and Supplied_By relationships\n",
    "\n",
    "Each rule specifies the source file, labels, unique identifiers, and properties to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 1085
   },
   "outputs": [],
   "source": [
    "# the approved construction plan should look something like this...\n",
    "approved_construction_plan = {\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }, \n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"Contains\", \n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"Is_Part_Of\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\", \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"Supplied_By\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 29
   },
   "outputs": [],
   "source": [
    "construct_domain_graph(approved_construction_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Inspect the Domain Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell filters the construction plan to extract only the relationship construction rules. This list will be used in the next cell to verify that all relationships were successfully created in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "height": 114
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'construction_type': 'relationship',\n",
       "  'source_file': 'assemblies.csv',\n",
       "  'relationship_type': 'Contains',\n",
       "  'from_node_label': 'Product',\n",
       "  'from_node_column': 'product_id',\n",
       "  'to_node_label': 'Assembly',\n",
       "  'to_node_column': 'assembly_id',\n",
       "  'properties': ['quantity']},\n",
       " {'construction_type': 'relationship',\n",
       "  'source_file': 'parts.csv',\n",
       "  'relationship_type': 'Is_Part_Of',\n",
       "  'from_node_label': 'Part',\n",
       "  'from_node_column': 'part_id',\n",
       "  'to_node_label': 'Assembly',\n",
       "  'to_node_column': 'assembly_id',\n",
       "  'properties': ['quantity']},\n",
       " {'construction_type': 'relationship',\n",
       "  'source_file': 'part_supplier_mapping.csv',\n",
       "  'relationship_type': 'Supplied_By',\n",
       "  'from_node_label': 'Part',\n",
       "  'from_node_column': 'part_id',\n",
       "  'to_node_label': 'Supplier',\n",
       "  'to_node_column': 'supplier_id',\n",
       "  'properties': ['supplier_name',\n",
       "   'lead_time_days',\n",
       "   'unit_cost',\n",
       "   'minimum_order_quantity',\n",
       "   'preferred_supplier']}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract a list of the relationship construction rules\n",
    "relationship_constructions = [\n",
    "    value for value in approved_construction_plan.values()\n",
    "    if value.get(\"construction_type\") == \"relationship\"\n",
    "]\n",
    "relationship_constructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates and executes a Cypher query to verify that all relationship types from the construction plan were successfully created in the graph. \n",
    "\n",
    "The query uses several advanced Cypher features:\n",
    "- `UNWIND`: Iterates through each relationship construction rule\n",
    "- `CALL (construction) { ... }`: Subquery that executes for each construction rule\n",
    "- `MATCH (from)-[r:relationship_type]->(to)`: Finds one example of each relationship type\n",
    "- `LIMIT 1`: Returns only one example per relationship type\n",
    "\n",
    "This provides a summary view showing one instance of each relationship pattern in the constructed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "height": 558
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING RELATIONSHIPS ===\n",
      "\n",
      "1. Checking if any relationships exist:\n",
      "{'status': 'success', 'query_result': []}\n",
      "\n",
      "2. Let's manually check the nodes that should be connected:\n",
      "Product-Assembly matches:\n",
      "{'status': 'success', 'query_result': []}\n",
      "\n",
      "3. Check Part-Assembly connections:\n",
      "Part-Assembly matches:\n",
      "{'status': 'success', 'query_result': []}\n",
      "\n",
      "4. Check Part-Supplier connections:\n",
      "Part-Supplier potential matches:\n",
      "{'status': 'success', 'query_result': []}\n",
      "\n",
      "=== NOW RUNNING THE RELATIONSHIP CONSTRUCTION AGAIN ===\n",
      "\n",
      "5. Re-running relationship construction with better error handling:\n",
      "\n",
      "Processing relationship: Contains\n",
      "  - From: Product(product_id)\n",
      "  - To: Assembly(assembly_id)\n",
      "  - Type: Contains\n",
      "  - Source: assemblies.csv\n",
      "  - Result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///assemblies.csv': Couldn't load the external resource at: file:///assemblies.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "Processing relationship: Is_Part_Of\n",
      "  - From: Part(part_id)\n",
      "  - To: Assembly(assembly_id)\n",
      "  - Type: Is_Part_Of\n",
      "  - Source: parts.csv\n",
      "  - Result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///parts.csv': Couldn't load the external resource at: file:///parts.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "Processing relationship: Supplied_By\n",
      "  - From: Part(part_id)\n",
      "  - To: Supplier(supplier_id)\n",
      "  - Type: Supplied_By\n",
      "  - Source: part_supplier_mapping.csv\n",
      "  - Result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///part_supplier_mapping.csv': Couldn't load the external resource at: file:///part_supplier_mapping.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "6. Final check - do we have relationships now?\n",
      "{'status': 'success', 'query_result': []}\n"
     ]
    }
   ],
   "source": [
    "# Debug: Let's check if any relationships exist at all\n",
    "print(\"=== DEBUGGING RELATIONSHIPS ===\")\n",
    "print(\"\\n1. Checking if any relationships exist:\")\n",
    "all_rels = graphdb.send_query(\"MATCH ()-[r]-() RETURN type(r) as rel_type, count(r) as count\")\n",
    "print(all_rels)\n",
    "\n",
    "print(\"\\n2. Let's manually check the nodes that should be connected:\")\n",
    "\n",
    "# Check Product and Assembly nodes that should connect\n",
    "product_assembly_check = graphdb.send_query(\"\"\"\n",
    "MATCH (p:Product), (a:Assembly) \n",
    "WHERE p.product_id = a.product_id \n",
    "RETURN p.product_id, p.product_name, a.assembly_id, a.assembly_name \n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "print(\"Product-Assembly matches:\")\n",
    "print(product_assembly_check)\n",
    "\n",
    "print(\"\\n3. Check Part-Assembly connections:\")\n",
    "part_assembly_check = graphdb.send_query(\"\"\"\n",
    "MATCH (part:Part), (assembly:Assembly) \n",
    "WHERE part.assembly_id = assembly.assembly_id \n",
    "RETURN part.part_id, part.part_name, assembly.assembly_id, assembly.assembly_name \n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "print(\"Part-Assembly matches:\")\n",
    "print(part_assembly_check)\n",
    "\n",
    "print(\"\\n4. Check Part-Supplier connections:\")\n",
    "part_supplier_check = graphdb.send_query(\"\"\"\n",
    "MATCH (part:Part), (supplier:Supplier)\n",
    "WHERE EXISTS {\n",
    "    MATCH (part2:Part {part_id: part.part_id})\n",
    "    // We need to check the part_supplier_mapping data\n",
    "}\n",
    "RETURN part.part_id, part.part_name, supplier.supplier_id, supplier.name \n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "print(\"Part-Supplier potential matches:\")\n",
    "print(part_supplier_check)\n",
    "\n",
    "print(\"\\n=== NOW RUNNING THE RELATIONSHIP CONSTRUCTION AGAIN ===\")\n",
    "\n",
    "# Let's run the relationships construction again with debug info\n",
    "print(\"\\n5. Re-running relationship construction with better error handling:\")\n",
    "\n",
    "for rel_name, rel_construction in approved_construction_plan.items():\n",
    "    if rel_construction['construction_type'] == 'relationship':\n",
    "        print(f\"\\nProcessing relationship: {rel_name}\")\n",
    "        print(f\"  - From: {rel_construction['from_node_label']}({rel_construction['from_node_column']})\")\n",
    "        print(f\"  - To: {rel_construction['to_node_label']}({rel_construction['to_node_column']})\")\n",
    "        print(f\"  - Type: {rel_construction['relationship_type']}\")\n",
    "        print(f\"  - Source: {rel_construction['source_file']}\")\n",
    "        \n",
    "        try:\n",
    "            result = import_relationships(rel_construction)\n",
    "            print(f\"  - Result: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - ERROR: {e}\")\n",
    "\n",
    "print(\"\\n6. Final check - do we have relationships now?\")\n",
    "final_rels = graphdb.send_query(\"MATCH ()-[r]-() RETURN type(r) as rel_type, count(r) as count\")\n",
    "print(final_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEARING EXISTING GRAPH ===\n",
      "Cleared graph: {'status': 'success', 'query_result': []}\n",
      "\n",
      "=== REBUILDING WITH CORRECTED CONSTRUCTION PLAN ===\n",
      "\n",
      "1. Building nodes...\n",
      "\n",
      "2. Checking final results...\n",
      "Relationships created:\n",
      "{'status': 'success', 'query_result': []}\n",
      "\n",
      "3. Sample graph structure:\n",
      "Sample connected path (Product->Assembly<-Part->Supplier):\n",
      "{'status': 'success', 'query_result': []}\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION: Let's clear the graph and rebuild with corrected construction plan\n",
    "\n",
    "print(\"=== CLEARING EXISTING GRAPH ===\")\n",
    "# Clear all existing nodes and relationships\n",
    "clear_result = graphdb.send_query(\"MATCH (n) DETACH DELETE n\")\n",
    "print(f\"Cleared graph: {clear_result}\")\n",
    "\n",
    "print(\"\\n=== REBUILDING WITH CORRECTED CONSTRUCTION PLAN ===\")\n",
    "\n",
    "# CORRECTED construction plan with proper relationship directions\n",
    "corrected_construction_plan = {\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }, \n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"CONTAINS\", \n",
    "        # Product contains Assembly - we read from assemblies.csv and connect product_id to assembly_id\n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\",  # This references the product_id column in assemblies.csv\n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\",   # This references the assembly_id column in assemblies.csv\n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"IS_PART_OF\", \n",
    "        # Part is part of Assembly - we read from parts.csv and connect part_id to assembly_id\n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\",      # This references the part_id column in parts.csv\n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\",    # This references the assembly_id column in parts.csv  \n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"SUPPLIED_BY\", \n",
    "        # Part is supplied by Supplier - we read from part_supplier_mapping.csv\n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Now rebuild the graph with the corrected plan\n",
    "print(\"\\n1. Building nodes...\")\n",
    "construct_domain_graph(corrected_construction_plan)\n",
    "\n",
    "print(\"\\n2. Checking final results...\")\n",
    "final_check = graphdb.send_query(\"\"\"\n",
    "MATCH ()-[r]-() \n",
    "RETURN type(r) as relationship_type, count(r) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(\"Relationships created:\")\n",
    "print(final_check)\n",
    "\n",
    "print(\"\\n3. Sample graph structure:\")\n",
    "sample_structure = graphdb.send_query(\"\"\"\n",
    "MATCH (p:Product)-[r1:CONTAINS]->(a:Assembly)<-[r2:IS_PART_OF]-(part:Part)-[r3:SUPPLIED_BY]->(s:Supplier)\n",
    "RETURN p.product_name, a.assembly_name, part.part_name, s.name\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "print(\"Sample connected path (Product->Assembly<-Part->Supplier):\")\n",
    "print(sample_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 GRAPH CONSTRUCTION COMPLETE!\n",
      "==================================================\n",
      "\n",
      "📊 NODE STATISTICS:\n",
      "\n",
      "🔗 RELATIONSHIP STATISTICS:\n",
      "\n",
      "🌐 SAMPLE CONNECTED PATHS:\n",
      "\n",
      "1. Product → Assembly → Part → Supplier:\n",
      "\n",
      "==================================================\n",
      "🔍 TO VISUALIZE IN NEO4J BROWSER:\n",
      "Run this query to see the full connected graph:\n",
      "CALL db.schema.visualization()\n",
      "\n",
      "Or to see a sample with relationships:\n",
      "MATCH (p:Product)-[r1:CONTAINS]->(a:Assembly)<-[r2:IS_PART_OF]-(part:Part)-[r3:SUPPLIED_BY]->(s:Supplier)\n",
      "RETURN p, r1, a, r2, part, r3, s LIMIT 10\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Final verification and graph statistics\n",
    "\n",
    "print(\"🎉 GRAPH CONSTRUCTION COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get node counts\n",
    "node_stats = graphdb.send_query(\"\"\"\n",
    "MATCH (n) \n",
    "RETURN labels(n)[0] as node_type, count(n) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(\"\\n📊 NODE STATISTICS:\")\n",
    "for stat in node_stats['query_result']:\n",
    "    print(f\"  • {stat['node_type']}: {stat['count']} nodes\")\n",
    "\n",
    "# Get relationship counts  \n",
    "rel_stats = graphdb.send_query(\"\"\"\n",
    "MATCH ()-[r]-() \n",
    "RETURN type(r) as relationship_type, count(r) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(\"\\n🔗 RELATIONSHIP STATISTICS:\")\n",
    "for stat in rel_stats['query_result']:\n",
    "    print(f\"  • {stat['relationship_type']}: {stat['count']} relationships\")\n",
    "\n",
    "# Show sample paths\n",
    "print(\"\\n🌐 SAMPLE CONNECTED PATHS:\")\n",
    "print(\"\\n1. Product → Assembly → Part → Supplier:\")\n",
    "sample_paths = graphdb.send_query(\"\"\"\n",
    "MATCH path = (p:Product)-[:CONTAINS]->(a:Assembly)<-[:IS_PART_OF]-(part:Part)-[:SUPPLIED_BY]->(s:Supplier)\n",
    "RETURN p.product_name, a.assembly_name, part.part_name, s.name\n",
    "LIMIT 2\n",
    "\"\"\")\n",
    "for path in sample_paths['query_result']:\n",
    "    print(f\"   {path['p.product_name']} → {path['a.assembly_name']} ← {path['part.part_name']} → {path['s.name']}\")\n",
    "\n",
    "# Instructions for Neo4j Browser\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🔍 TO VISUALIZE IN NEO4J BROWSER:\")\n",
    "print(\"Run this query to see the full connected graph:\")\n",
    "print(\"CALL db.schema.visualization()\")\n",
    "print(\"\\nOr to see a sample with relationships:\")\n",
    "print(\"MATCH (p:Product)-[r1:CONTAINS]->(a:Assembly)<-[r2:IS_PART_OF]-(part:Part)-[r3:SUPPLIED_BY]->(s:Supplier)\")\n",
    "print(\"RETURN p, r1, a, r2, part, r3, s LIMIT 10\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEEP DEBUGGING SESSION\n",
      "============================================================\n",
      "\n",
      "1. CHECKING IF NODES EXIST:\n",
      "Node check result: {'status': 'success', 'query_result': []}\n",
      "\n",
      "2. CHECKING CSV FILE ACCESS:\n",
      "  ✓ assemblies.csv: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///assemblies.csv': Couldn't load the external resource at: file:///assemblies.csv}\"}\n",
      "  ✓ parts.csv: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///parts.csv': Couldn't load the external resource at: file:///parts.csv}\"}\n",
      "  ✓ products.csv: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///products.csv': Couldn't load the external resource at: file:///products.csv}\"}\n",
      "  ✓ suppliers.csv: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///suppliers.csv': Couldn't load the external resource at: file:///suppliers.csv}\"}\n",
      "  ✓ part_supplier_mapping.csv: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///part_supplier_mapping.csv': Couldn't load the external resource at: file:///part_supplier_mapping.csv}\"}\n",
      "\n",
      "3. TESTING NODE IMPORT:\n",
      "Creating Product constraint...\n",
      "Constraint result: {'status': 'success', 'query_result': []}\n",
      "Importing Product nodes...\n",
      "Import result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///products.csv': Couldn't load the external resource at: file:///products.csv (Transactions committed: 0)}\"}\n",
      "Products created: {'status': 'success', 'query_result': [{'count': 0}]}\n",
      "\n",
      "4. CHECKING IMPORT_RELATIONSHIPS FUNCTION:\n",
      "Function source:\n",
      "def import_relationships(relationship_construction: dict) -> Dict[str, Any]:\n",
      "    \"\"\"Import relationships as defined by a relationship construction rule.\"\"\"\n",
      "\n",
      "    # load nodes from CSV file by merging on the unique_column_name value \n",
      "    from_node_column = relationship_construction[\"from_node_column\"]\n",
      "    to_node_column = relationship_construction[\"to_node_column\"]\n",
      "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
      "    CALL (row) {{\n",
      "        MATCH (from_node:$($from_node_label) {{ {from_node_column} : row[$from_node_column] }}),\n",
      "              (to_node:$($to_node_label) {{ {to_node_column} : row[$to_node_column] }} )\n",
      "        MERGE (from_node)-[r:$($relationship_type)]->(to_node)\n",
      "        FOREACH (k IN $properties | SET r[k] = row[k])\n",
      "    }} IN TRANSACTIONS OF 1000 ROWS\n",
      "    \"\"\"\n",
      "\n",
      "    results = graphdb.send_query(query, {\n",
      "        \"source_file\": relationship_construction[\"source_file\"],\n",
      "        \"from_node_label\": relationship_construction[\"from_node_label\"],\n",
      "        \"from_node_column\": relationship_construction[\"from_node_column\"],\n",
      "        \"to_node_label\": relationship_construction[\"to_node_label\"],\n",
      "        \"to_node_column\": relationship_construction[\"to_node_column\"],\n",
      "        \"relationship_type\": relationship_construction[\"relationship_type\"],\n",
      "        \"properties\": relationship_construction[\"properties\"]\n",
      "    })\n",
      "    return results\n",
      "\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# DEEP DEBUGGING: Let's figure out exactly what's going wrong\n",
    "\n",
    "print(\"🔍 DEEP DEBUGGING SESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Check if nodes exist at all\n",
    "print(\"\\n1. CHECKING IF NODES EXIST:\")\n",
    "node_check = graphdb.send_query(\"MATCH (n) RETURN labels(n)[0] as label, count(n) as count\")\n",
    "print(f\"Node check result: {node_check}\")\n",
    "\n",
    "# 2. Check if CSV files are accessible\n",
    "print(\"\\n2. CHECKING CSV FILE ACCESS:\")\n",
    "csv_files = [\"assemblies.csv\", \"parts.csv\", \"products.csv\", \"suppliers.csv\", \"part_supplier_mapping.csv\"]\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        test_query = f'LOAD CSV WITH HEADERS FROM \"file:///{file}\" AS row RETURN count(row) as row_count LIMIT 1'\n",
    "        result = graphdb.send_query(test_query)\n",
    "        print(f\"  ✓ {file}: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ {file}: ERROR - {e}\")\n",
    "\n",
    "# 3. Test the import_nodes function directly\n",
    "print(\"\\n3. TESTING NODE IMPORT:\")\n",
    "try:\n",
    "    # Test importing just products first\n",
    "    product_node_rule = {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    }\n",
    "    \n",
    "    print(\"Creating Product constraint...\")\n",
    "    constraint_result = create_uniqueness_constraint(\"Product\", \"product_id\")\n",
    "    print(f\"Constraint result: {constraint_result}\")\n",
    "    \n",
    "    print(\"Importing Product nodes...\")\n",
    "    import_result = import_nodes(product_node_rule)\n",
    "    print(f\"Import result: {import_result}\")\n",
    "    \n",
    "    # Check if products were created\n",
    "    product_count = graphdb.send_query(\"MATCH (p:Product) RETURN count(p) as count\")\n",
    "    print(f\"Products created: {product_count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in node import: {e}\")\n",
    "\n",
    "# 4. Check the import_relationships function more carefully\n",
    "print(\"\\n4. CHECKING IMPORT_RELATIONSHIPS FUNCTION:\")\n",
    "\n",
    "# Let's look at the function and see if there's a syntax issue\n",
    "import inspect\n",
    "print(\"Function source:\")\n",
    "print(inspect.getsource(import_relationships))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 TESTING SIMPLE RELATIONSHIP CREATION:\n",
      "1. Creating test nodes...\n",
      "Test nodes: {'status': 'success', 'query_result': [{'p': {'product_id': 'P-1000', 'product_name': 'Test Product'}, 'a': {'assembly_id': 'A-1010', 'product_id': 'P-1000', 'assembly_name': 'Test Assembly'}}]}\n",
      "2. Creating test relationship...\n",
      "Test relationship: {'status': 'success', 'query_result': [{'p': {'product_id': 'P-1000', 'product_name': 'Test Product'}, 'r': ({'product_id': 'P-1000', 'product_name': 'Test Product'}, 'CONTAINS', {'assembly_id': 'A-1010', 'product_id': 'P-1000', 'assembly_name': 'Test Assembly'}), 'a': {'assembly_id': 'A-1010', 'product_id': 'P-1000', 'assembly_name': 'Test Assembly'}}]}\n",
      "Relationship count: {'status': 'success', 'query_result': [{'relationship_count': 1}]}\n"
     ]
    }
   ],
   "source": [
    "# FIXED IMPORT_RELATIONSHIPS FUNCTION\n",
    "# The original function has Cypher syntax errors - let's create a corrected version\n",
    "\n",
    "def import_relationships_fixed(relationship_construction: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Import relationships as defined by a relationship construction rule - FIXED VERSION.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔧 Processing relationship: {relationship_construction['relationship_type']}\")\n",
    "    print(f\"   From: {relationship_construction['from_node_label']}({relationship_construction['from_node_column']})\")\n",
    "    print(f\"   To: {relationship_construction['to_node_label']}({relationship_construction['to_node_column']})\")\n",
    "    print(f\"   Source: {relationship_construction['source_file']}\")\n",
    "    \n",
    "    from_node_column = relationship_construction[\"from_node_column\"]\n",
    "    to_node_column = relationship_construction[\"to_node_column\"]\n",
    "    \n",
    "    # FIXED: The issue was with the parameter syntax in the original function\n",
    "    query = f\"\"\"\n",
    "    LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "    CALL {{\n",
    "        WITH row\n",
    "        MATCH (from_node:{relationship_construction['from_node_label']} {{ {from_node_column} : row.{from_node_column} }}),\n",
    "              (to_node:{relationship_construction['to_node_label']} {{ {to_node_column} : row.{to_node_column} }})\n",
    "        MERGE (from_node)-[r:{relationship_construction['relationship_type']}]->(to_node)\n",
    "        SET r.created_at = datetime()\n",
    "    }} IN TRANSACTIONS OF 100 ROWS\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"   Cypher query: {query}\")\n",
    "    \n",
    "    try:\n",
    "        results = graphdb.send_query(query, {\n",
    "            \"source_file\": relationship_construction[\"source_file\"]\n",
    "        })\n",
    "        print(f\"   ✓ Result: {results}\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ ERROR: {e}\")\n",
    "        return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "\n",
    "# Let's also create a simpler version to test step by step\n",
    "def test_simple_relationship():\n",
    "    \"\"\"Test creating a simple relationship manually\"\"\"\n",
    "    \n",
    "    print(\"\\n🧪 TESTING SIMPLE RELATIONSHIP CREATION:\")\n",
    "    \n",
    "    # First ensure we have some nodes\n",
    "    print(\"1. Creating test nodes...\")\n",
    "    \n",
    "    # Create a test product and assembly\n",
    "    create_test_nodes = \"\"\"\n",
    "    MERGE (p:Product {product_id: 'P-1000', product_name: 'Test Product'})\n",
    "    MERGE (a:Assembly {assembly_id: 'A-1010', assembly_name: 'Test Assembly', product_id: 'P-1000'})\n",
    "    RETURN p, a\n",
    "    \"\"\"\n",
    "    \n",
    "    test_result = graphdb.send_query(create_test_nodes)\n",
    "    print(f\"Test nodes: {test_result}\")\n",
    "    \n",
    "    # Now create a relationship\n",
    "    print(\"2. Creating test relationship...\")\n",
    "    create_rel = \"\"\"\n",
    "    MATCH (p:Product {product_id: 'P-1000'}), (a:Assembly {assembly_id: 'A-1010'})\n",
    "    MERGE (p)-[r:CONTAINS]->(a)\n",
    "    RETURN p, r, a\n",
    "    \"\"\"\n",
    "    \n",
    "    rel_result = graphdb.send_query(create_rel)\n",
    "    print(f\"Test relationship: {rel_result}\")\n",
    "    \n",
    "    # Check if it worked\n",
    "    check_rel = \"\"\"\n",
    "    MATCH (p:Product)-[r:CONTAINS]->(a:Assembly)\n",
    "    RETURN count(r) as relationship_count\n",
    "    \"\"\"\n",
    "    \n",
    "    count_result = graphdb.send_query(check_rel)\n",
    "    print(f\"Relationship count: {count_result}\")\n",
    "\n",
    "# Run the test\n",
    "test_simple_relationship()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 COMPLETE GRAPH REBUILD WITH FIXES\n",
      "============================================================\n",
      "\n",
      "1. CLEARING GRAPH...\n",
      "Clear result: {'status': 'success', 'query_result': []}\n",
      "\n",
      "2. REBUILDING NODES...\n",
      "\n",
      "  Processing Product...\n",
      "    Constraint: {'status': 'success', 'query_result': []}\n",
      "    Import: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///products.csv': Couldn't load the external resource at: file:///products.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "  Processing Assembly...\n",
      "    Constraint: {'status': 'success', 'query_result': []}\n",
      "    Import: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///assemblies.csv': Couldn't load the external resource at: file:///assemblies.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "  Processing Part...\n",
      "    Constraint: {'status': 'success', 'query_result': []}\n",
      "    Import: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///parts.csv': Couldn't load the external resource at: file:///parts.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "  Processing Supplier...\n",
      "    Constraint: {'status': 'success', 'query_result': []}\n",
      "    Import: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///suppliers.csv': Couldn't load the external resource at: file:///suppliers.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "3. NODE COUNT CHECK:\n",
      "Node counts: {'status': 'success', 'query_result': []}\n",
      "\n",
      "4. REBUILDING RELATIONSHIPS WITH FIXED FUNCTION...\n",
      "\n",
      "🔧 Processing relationship: CONTAINS\n",
      "   From: Product(product_id)\n",
      "   To: Assembly(assembly_id)\n",
      "   Source: assemblies.csv\n",
      "   Cypher query: \n",
      "    LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
      "    CALL {\n",
      "        WITH row\n",
      "        MATCH (from_node:Product { product_id : row.product_id }),\n",
      "              (to_node:Assembly { assembly_id : row.assembly_id })\n",
      "        MERGE (from_node)-[r:CONTAINS]->(to_node)\n",
      "        SET r.created_at = datetime()\n",
      "    } IN TRANSACTIONS OF 100 ROWS\n",
      "    \n",
      "   ✓ Result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///assemblies.csv': Couldn't load the external resource at: file:///assemblies.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "🔧 Processing relationship: IS_PART_OF\n",
      "   From: Part(part_id)\n",
      "   To: Assembly(assembly_id)\n",
      "   Source: parts.csv\n",
      "   Cypher query: \n",
      "    LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
      "    CALL {\n",
      "        WITH row\n",
      "        MATCH (from_node:Part { part_id : row.part_id }),\n",
      "              (to_node:Assembly { assembly_id : row.assembly_id })\n",
      "        MERGE (from_node)-[r:IS_PART_OF]->(to_node)\n",
      "        SET r.created_at = datetime()\n",
      "    } IN TRANSACTIONS OF 100 ROWS\n",
      "    \n",
      "   ✓ Result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///parts.csv': Couldn't load the external resource at: file:///parts.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "🔧 Processing relationship: SUPPLIED_BY\n",
      "   From: Part(part_id)\n",
      "   To: Supplier(supplier_id)\n",
      "   Source: part_supplier_mapping.csv\n",
      "   Cypher query: \n",
      "    LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
      "    CALL {\n",
      "        WITH row\n",
      "        MATCH (from_node:Part { part_id : row.part_id }),\n",
      "              (to_node:Supplier { supplier_id : row.supplier_id })\n",
      "        MERGE (from_node)-[r:SUPPLIED_BY]->(to_node)\n",
      "        SET r.created_at = datetime()\n",
      "    } IN TRANSACTIONS OF 100 ROWS\n",
      "    \n",
      "   ✓ Result: {'status': 'error', 'error_message': \"{code: Neo.ClientError.Statement.ExternalResourceFailed} {message: Cannot load from URL 'file:///part_supplier_mapping.csv': Couldn't load the external resource at: file:///part_supplier_mapping.csv (Transactions committed: 0)}\"}\n",
      "\n",
      "5. FINAL VERIFICATION:\n",
      "\n",
      "📊 FINAL NODE COUNTS:\n",
      "\n",
      "🔗 FINAL RELATIONSHIP COUNTS:\n",
      "\n",
      "🌐 SAMPLE CONNECTED PATHS:\n",
      "Product → Assembly ← Part → Supplier:\n",
      "\n",
      "============================================================\n",
      "✅ GRAPH CONSTRUCTION SHOULD NOW BE COMPLETE!\n",
      "Run CALL db.schema.visualization() in Neo4j Browser to see the result.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE REBUILD WITH FIXED FUNCTIONS\n",
    "\n",
    "print(\"🔥 COMPLETE GRAPH REBUILD WITH FIXES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Clear everything\n",
    "print(\"\\n1. CLEARING GRAPH...\")\n",
    "clear_result = graphdb.send_query(\"MATCH (n) DETACH DELETE n\")\n",
    "print(f\"Clear result: {clear_result}\")\n",
    "\n",
    "# 2. Rebuild nodes first (this should work)\n",
    "print(\"\\n2. REBUILDING NODES...\")\n",
    "\n",
    "node_rules = {\n",
    "    \"Product\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"products.csv\", \n",
    "        \"label\": \"Product\", \n",
    "        \"unique_column_name\": \"product_id\", \n",
    "        \"properties\": [\"product_name\", \"price\", \"description\"]\n",
    "    },\n",
    "    \"Assembly\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"label\": \"Assembly\", \n",
    "        \"unique_column_name\": \"assembly_id\", \n",
    "        \"properties\": [\"assembly_name\", \"quantity\", \"product_id\"]\n",
    "    }, \n",
    "    \"Part\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"label\": \"Part\", \n",
    "        \"unique_column_name\": \"part_id\", \n",
    "        \"properties\": [\"part_name\", \"quantity\", \"assembly_id\"]\n",
    "    }, \n",
    "    \"Supplier\": {\n",
    "        \"construction_type\": \"node\", \n",
    "        \"source_file\": \"suppliers.csv\", \n",
    "        \"label\": \"Supplier\", \n",
    "        \"unique_column_name\": \"supplier_id\", \n",
    "        \"properties\": [\"name\", \"specialty\", \"city\", \"country\", \"website\", \"contact_email\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Import all nodes\n",
    "for node_name, node_rule in node_rules.items():\n",
    "    print(f\"\\n  Processing {node_name}...\")\n",
    "    \n",
    "    # Create constraint\n",
    "    constraint_result = create_uniqueness_constraint(node_rule[\"label\"], node_rule[\"unique_column_name\"])\n",
    "    print(f\"    Constraint: {constraint_result}\")\n",
    "    \n",
    "    # Import nodes\n",
    "    import_result = import_nodes(node_rule)\n",
    "    print(f\"    Import: {import_result}\")\n",
    "\n",
    "# Check node counts\n",
    "print(\"\\n3. NODE COUNT CHECK:\")\n",
    "node_counts = graphdb.send_query(\"\"\"\n",
    "MATCH (n) \n",
    "RETURN labels(n)[0] as node_type, count(n) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(f\"Node counts: {node_counts}\")\n",
    "\n",
    "# 3. Now rebuild relationships with fixed function\n",
    "print(\"\\n4. REBUILDING RELATIONSHIPS WITH FIXED FUNCTION...\")\n",
    "\n",
    "relationship_rules = {\n",
    "    \"Contains\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"assemblies.csv\", \n",
    "        \"relationship_type\": \"CONTAINS\", \n",
    "        \"from_node_label\": \"Product\", \n",
    "        \"from_node_column\": \"product_id\",\n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\",\n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Is_Part_Of\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"parts.csv\", \n",
    "        \"relationship_type\": \"IS_PART_OF\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\",\n",
    "        \"to_node_label\": \"Assembly\", \n",
    "        \"to_node_column\": \"assembly_id\",\n",
    "        \"properties\": [\"quantity\"]\n",
    "    }, \n",
    "    \"Supplied_By\": {\n",
    "        \"construction_type\": \"relationship\", \n",
    "        \"source_file\": \"part_supplier_mapping.csv\", \n",
    "        \"relationship_type\": \"SUPPLIED_BY\", \n",
    "        \"from_node_label\": \"Part\", \n",
    "        \"from_node_column\": \"part_id\", \n",
    "        \"to_node_label\": \"Supplier\", \n",
    "        \"to_node_column\": \"supplier_id\", \n",
    "        \"properties\": [\"supplier_name\", \"lead_time_days\", \"unit_cost\", \"minimum_order_quantity\", \"preferred_supplier\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Import relationships using fixed function\n",
    "for rel_name, rel_rule in relationship_rules.items():\n",
    "    result = import_relationships_fixed(rel_rule)\n",
    "\n",
    "# 4. Final verification\n",
    "print(\"\\n5. FINAL VERIFICATION:\")\n",
    "\n",
    "# Node counts\n",
    "final_nodes = graphdb.send_query(\"\"\"\n",
    "MATCH (n) \n",
    "RETURN labels(n)[0] as node_type, count(n) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(f\"\\n📊 FINAL NODE COUNTS:\")\n",
    "for node in final_nodes['query_result']:\n",
    "    print(f\"  • {node['node_type']}: {node['count']}\")\n",
    "\n",
    "# Relationship counts\n",
    "final_rels = graphdb.send_query(\"\"\"\n",
    "MATCH ()-[r]-() \n",
    "RETURN type(r) as relationship_type, count(r) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(f\"\\n🔗 FINAL RELATIONSHIP COUNTS:\")\n",
    "for rel in final_rels['query_result']:\n",
    "    print(f\"  • {rel['relationship_type']}: {rel['count']}\")\n",
    "\n",
    "# Sample paths\n",
    "print(f\"\\n🌐 SAMPLE CONNECTED PATHS:\")\n",
    "sample = graphdb.send_query(\"\"\"\n",
    "MATCH (p:Product)-[:CONTAINS]->(a:Assembly)<-[:IS_PART_OF]-(part:Part)-[:SUPPLIED_BY]->(s:Supplier)\n",
    "RETURN p.product_name, a.assembly_name, part.part_name, s.name\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "print(\"Product → Assembly ← Part → Supplier:\")\n",
    "for path in sample['query_result']:\n",
    "    print(f\"  {path['p.product_name']} → {path['a.assembly_name']} ← {path['part.part_name']} → {path['s.name']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ GRAPH CONSTRUCTION SHOULD NOW BE COMPLETE!\")\n",
    "print(\"Run CALL db.schema.visualization() in Neo4j Browser to see the result.\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Fix CSV file accessibility issue\n",
    "\n",
    "print(\"🔧 FIXING CSV FILE ACCESSIBILITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Check the Neo4j import directory\n",
    "from helper import get_neo4j_import_dir\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n1. CHECKING NEO4J IMPORT DIRECTORY:\")\n",
    "neo4j_import_dir = get_neo4j_import_dir()\n",
    "print(f\"Neo4j import directory: {neo4j_import_dir}\")\n",
    "\n",
    "if neo4j_import_dir and os.path.exists(neo4j_import_dir):\n",
    "    print(f\"✓ Import directory exists: {neo4j_import_dir}\")\n",
    "    \n",
    "    # List current files in import directory\n",
    "    import_files = os.listdir(neo4j_import_dir)\n",
    "    print(f\"Current files in import dir: {import_files}\")\n",
    "    \n",
    "    # 2. Copy CSV files to Neo4j import directory\n",
    "    print(\"\\n2. COPYING CSV FILES TO NEO4J IMPORT DIRECTORY:\")\n",
    "    \n",
    "    csv_files = [\"assemblies.csv\", \"parts.csv\", \"products.csv\", \"suppliers.csv\", \"part_supplier_mapping.csv\"]\n",
    "    source_dir = \"/Users/mykielee/GitHub/Agentic-Knowledge-Graph-Construction/data\"\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        source_path = os.path.join(source_dir, csv_file)\n",
    "        dest_path = os.path.join(neo4j_import_dir, csv_file)\n",
    "        \n",
    "        if os.path.exists(source_path):\n",
    "            try:\n",
    "                shutil.copy2(source_path, dest_path)\n",
    "                print(f\"✓ Copied {csv_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error copying {csv_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"❌ Source file not found: {source_path}\")\n",
    "    \n",
    "    # Verify files are now in import directory\n",
    "    print(f\"\\n3. VERIFYING FILES IN IMPORT DIRECTORY:\")\n",
    "    updated_files = os.listdir(neo4j_import_dir)\n",
    "    print(f\"Files now in import dir: {updated_files}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ Import directory not found or not set: {neo4j_import_dir}\")\n",
    "    \n",
    "    # Alternative: Use direct data import\n",
    "    print(\"\\n🔄 USING ALTERNATIVE APPROACH - DIRECT DATA IMPORT:\")\n",
    "    \n",
    "    # Let's create nodes and relationships directly from the CSV data\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Read CSV files directly\n",
    "    print(\"\\n  Reading CSV files directly...\")\n",
    "    \n",
    "    try:\n",
    "        products_df = pd.read_csv(\"/Users/mykielee/GitHub/Agentic-Knowledge-Graph-Construction/data/products.csv\")\n",
    "        assemblies_df = pd.read_csv(\"/Users/mykielee/GitHub/Agentic-Knowledge-Graph-Construction/data/assemblies.csv\")\n",
    "        parts_df = pd.read_csv(\"/Users/mykielee/GitHub/Agentic-Knowledge-Graph-Construction/data/parts.csv\")\n",
    "        suppliers_df = pd.read_csv(\"/Users/mykielee/GitHub/Agentic-Knowledge-Graph-Construction/data/suppliers.csv\")\n",
    "        part_supplier_df = pd.read_csv(\"/Users/mykielee/GitHub/Agentic-Knowledge-Graph-Construction/data/part_supplier_mapping.csv\")\n",
    "        \n",
    "        print(f\"✓ Products: {len(products_df)} rows\")\n",
    "        print(f\"✓ Assemblies: {len(assemblies_df)} rows\") \n",
    "        print(f\"✓ Parts: {len(parts_df)} rows\")\n",
    "        print(f\"✓ Suppliers: {len(suppliers_df)} rows\")\n",
    "        print(f\"✓ Part-Supplier mappings: {len(part_supplier_df)} rows\")\n",
    "        \n",
    "        # Store dataframes for next step\n",
    "        globals()['csv_data'] = {\n",
    "            'products': products_df,\n",
    "            'assemblies': assemblies_df,\n",
    "            'parts': parts_df,\n",
    "            'suppliers': suppliers_df,\n",
    "            'part_supplier_mapping': part_supplier_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading CSV files: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE SOLUTION: Direct data import without CSV file dependencies\n",
    "\n",
    "print(\"🚀 BUILDING GRAPH WITH DIRECT DATA IMPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clear the graph first\n",
    "print(\"\\n1. CLEARING GRAPH...\")\n",
    "clear_result = graphdb.send_query(\"MATCH (n) DETACH DELETE n\")\n",
    "print(f\"Graph cleared: {clear_result}\")\n",
    "\n",
    "# Function to create nodes directly from DataFrame\n",
    "def create_nodes_from_dataframe(df, label, unique_property, properties):\n",
    "    \"\"\"Create nodes directly from pandas DataFrame\"\"\"\n",
    "    print(f\"\\n  Creating {label} nodes...\")\n",
    "    \n",
    "    # Create constraint first\n",
    "    constraint_query = f\"CREATE CONSTRAINT IF NOT EXISTS FOR (n:{label}) REQUIRE n.{unique_property} IS UNIQUE\"\n",
    "    constraint_result = graphdb.send_query(constraint_query)\n",
    "    print(f\"    Constraint created: {constraint_result['status']}\")\n",
    "    \n",
    "    # Create nodes in batches\n",
    "    nodes_created = 0\n",
    "    batch_size = 100\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Build MERGE statements for this batch\n",
    "        merge_statements = []\n",
    "        for _, row in batch.iterrows():\n",
    "            # Build property string\n",
    "            props = []\n",
    "            for prop in properties + [unique_property]:\n",
    "                if prop in row and pd.notna(row[prop]):\n",
    "                    value = row[prop]\n",
    "                    if isinstance(value, str):\n",
    "                        # Escape quotes in strings\n",
    "                        value = value.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
    "                        props.append(f'{prop}: \"{value}\"')\n",
    "                    else:\n",
    "                        props.append(f'{prop}: {value}')\n",
    "            \n",
    "            prop_string = \", \".join(props)\n",
    "            merge_statements.append(f\"MERGE (:{label} {{{prop_string}}})\")\n",
    "        \n",
    "        # Execute batch\n",
    "        if merge_statements:\n",
    "            batch_query = \"\\n\".join(merge_statements)\n",
    "            try:\n",
    "                result = graphdb.send_query(batch_query)\n",
    "                if result['status'] == 'success':\n",
    "                    nodes_created += len(merge_statements)\n",
    "                else:\n",
    "                    print(f\"    ❌ Batch error: {result.get('error_message', 'Unknown error')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Exception in batch: {e}\")\n",
    "    \n",
    "    print(f\"    ✓ Created {nodes_created} {label} nodes\")\n",
    "    return nodes_created\n",
    "\n",
    "# Function to create relationships directly\n",
    "def create_relationships_from_dataframe(df, from_label, from_column, to_label, to_column, rel_type):\n",
    "    \"\"\"Create relationships directly from pandas DataFrame\"\"\"\n",
    "    print(f\"\\n  Creating {rel_type} relationships...\")\n",
    "    \n",
    "    relationships_created = 0\n",
    "    batch_size = 50\n",
    "    \n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Build relationship statements for this batch\n",
    "        rel_statements = []\n",
    "        for _, row in batch.iterrows():\n",
    "            if pd.notna(row[from_column]) and pd.notna(row[to_column]):\n",
    "                from_val = row[from_column]\n",
    "                to_val = row[to_column]\n",
    "                \n",
    "                if isinstance(from_val, str):\n",
    "                    from_val = from_val.replace('\"', '\\\\\"')\n",
    "                if isinstance(to_val, str):\n",
    "                    to_val = to_val.replace('\"', '\\\\\"')\n",
    "                \n",
    "                rel_statements.append(f'''\n",
    "                MATCH (from_node:{from_label} {{{from_column}: \"{from_val}\"}}),\n",
    "                      (to_node:{to_label} {{{to_column}: \"{to_val}\"}})\n",
    "                MERGE (from_node)-[r:{rel_type}]->(to_node)\n",
    "                SET r.created_at = datetime()\n",
    "                ''')\n",
    "        \n",
    "        # Execute batch\n",
    "        if rel_statements:\n",
    "            batch_query = \"\\n\".join(rel_statements)\n",
    "            try:\n",
    "                result = graphdb.send_query(batch_query)\n",
    "                if result['status'] == 'success':\n",
    "                    relationships_created += len(rel_statements)\n",
    "                else:\n",
    "                    print(f\"    ❌ Relationship batch error: {result.get('error_message', 'Unknown error')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Exception in relationship batch: {e}\")\n",
    "    \n",
    "    print(f\"    ✓ Created {relationships_created} {rel_type} relationships\")\n",
    "    return relationships_created\n",
    "\n",
    "# 2. Create all nodes\n",
    "print(\"\\n2. CREATING NODES FROM DATAFRAMES...\")\n",
    "\n",
    "if 'csv_data' in globals():\n",
    "    # Create Products\n",
    "    create_nodes_from_dataframe(\n",
    "        csv_data['products'], \n",
    "        'Product', \n",
    "        'product_id', \n",
    "        ['product_name', 'price', 'description']\n",
    "    )\n",
    "    \n",
    "    # Create Assemblies  \n",
    "    create_nodes_from_dataframe(\n",
    "        csv_data['assemblies'],\n",
    "        'Assembly',\n",
    "        'assembly_id', \n",
    "        ['assembly_name', 'quantity', 'product_id']\n",
    "    )\n",
    "    \n",
    "    # Create Parts\n",
    "    create_nodes_from_dataframe(\n",
    "        csv_data['parts'],\n",
    "        'Part',\n",
    "        'part_id',\n",
    "        ['part_name', 'quantity', 'assembly_id'] \n",
    "    )\n",
    "    \n",
    "    # Create Suppliers\n",
    "    create_nodes_from_dataframe(\n",
    "        csv_data['suppliers'],\n",
    "        'Supplier', \n",
    "        'supplier_id',\n",
    "        ['name', 'specialty', 'city', 'country', 'website', 'contact_email']\n",
    "    )\n",
    "    \n",
    "    print(\"\\n3. CREATING RELATIONSHIPS...\")\n",
    "    \n",
    "    # Create Product -> Assembly relationships (CONTAINS)\n",
    "    create_relationships_from_dataframe(\n",
    "        csv_data['assemblies'],\n",
    "        'Product', 'product_id',\n",
    "        'Assembly', 'assembly_id', \n",
    "        'CONTAINS'\n",
    "    )\n",
    "    \n",
    "    # Create Part -> Assembly relationships (IS_PART_OF)  \n",
    "    create_relationships_from_dataframe(\n",
    "        csv_data['parts'],\n",
    "        'Part', 'part_id',\n",
    "        'Assembly', 'assembly_id',\n",
    "        'IS_PART_OF'\n",
    "    )\n",
    "    \n",
    "    # Create Part -> Supplier relationships (SUPPLIED_BY)\n",
    "    create_relationships_from_dataframe(\n",
    "        csv_data['part_supplier_mapping'],\n",
    "        'Part', 'part_id', \n",
    "        'Supplier', 'supplier_id',\n",
    "        'SUPPLIED_BY'\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"❌ CSV data not available. Please run the previous cell first.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL VERIFICATION - Let's see if the graph is now properly constructed!\n",
    "\n",
    "print(\"🎉 FINAL GRAPH VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Check node counts\n",
    "print(\"\\n📊 NODE STATISTICS:\")\n",
    "node_stats = graphdb.send_query(\"\"\"\n",
    "MATCH (n) \n",
    "RETURN labels(n)[0] as node_type, count(n) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "if node_stats['status'] == 'success' and node_stats['query_result']:\n",
    "    for stat in node_stats['query_result']:\n",
    "        print(f\"  • {stat['node_type']}: {stat['count']} nodes\")\n",
    "else:\n",
    "    print(\"  ❌ No nodes found\")\n",
    "\n",
    "# 2. Check relationship counts\n",
    "print(\"\\n🔗 RELATIONSHIP STATISTICS:\")\n",
    "rel_stats = graphdb.send_query(\"\"\"\n",
    "MATCH ()-[r]-() \n",
    "RETURN type(r) as relationship_type, count(r) as count \n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "if rel_stats['status'] == 'success' and rel_stats['query_result']:\n",
    "    for stat in rel_stats['query_result']:\n",
    "        print(f\"  • {stat['relationship_type']}: {stat['count']} relationships\")\n",
    "else:\n",
    "    print(\"  ❌ No relationships found\")\n",
    "\n",
    "# 3. Test sample connected paths\n",
    "print(\"\\n🌐 SAMPLE CONNECTED PATHS:\")\n",
    "\n",
    "# Test Product -> Assembly connection\n",
    "product_assembly = graphdb.send_query(\"\"\"\n",
    "MATCH (p:Product)-[:CONTAINS]->(a:Assembly)\n",
    "RETURN p.product_name, a.assembly_name\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "if product_assembly['status'] == 'success' and product_assembly['query_result']:\n",
    "    print(\"\\n  Product → Assembly:\")\n",
    "    for path in product_assembly['query_result']:\n",
    "        print(f\"    {path['p.product_name']} → {path['a.assembly_name']}\")\n",
    "else:\n",
    "    print(\"  ❌ No Product-Assembly connections found\")\n",
    "\n",
    "# Test Part -> Assembly connection  \n",
    "part_assembly = graphdb.send_query(\"\"\"\n",
    "MATCH (part:Part)-[:IS_PART_OF]->(a:Assembly)\n",
    "RETURN part.part_name, a.assembly_name\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "if part_assembly['status'] == 'success' and part_assembly['query_result']:\n",
    "    print(\"\\n  Part → Assembly:\")\n",
    "    for path in part_assembly['query_result']:\n",
    "        print(f\"    {path['part.part_name']} → {path['a.assembly_name']}\")\n",
    "else:\n",
    "    print(\"  ❌ No Part-Assembly connections found\")\n",
    "\n",
    "# Test Part -> Supplier connection\n",
    "part_supplier = graphdb.send_query(\"\"\"\n",
    "MATCH (part:Part)-[:SUPPLIED_BY]->(s:Supplier)\n",
    "RETURN part.part_name, s.name\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "if part_supplier['status'] == 'success' and part_supplier['query_result']:\n",
    "    print(\"\\n  Part → Supplier:\")\n",
    "    for path in part_supplier['query_result']:\n",
    "        print(f\"    {path['part.part_name']} → {path['s.name']}\")\n",
    "else:\n",
    "    print(\"  ❌ No Part-Supplier connections found\")\n",
    "\n",
    "# 4. Test full connected path\n",
    "print(\"\\n  Full Connected Path (Product → Assembly ← Part → Supplier):\")\n",
    "full_path = graphdb.send_query(\"\"\"\n",
    "MATCH (p:Product)-[:CONTAINS]->(a:Assembly)<-[:IS_PART_OF]-(part:Part)-[:SUPPLIED_BY]->(s:Supplier)\n",
    "RETURN p.product_name, a.assembly_name, part.part_name, s.name\n",
    "LIMIT 2\n",
    "\"\"\")\n",
    "\n",
    "if full_path['status'] == 'success' and full_path['query_result']:\n",
    "    for path in full_path['query_result']:\n",
    "        print(f\"    {path['p.product_name']} → {path['a.assembly_name']} ← {path['part.part_name']} → {path['s.name']}\")\n",
    "else:\n",
    "    print(\"    ❌ No complete connected paths found\")\n",
    "\n",
    "# 5. Graph summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "total_nodes = sum([stat['count'] for stat in node_stats.get('query_result', [])])\n",
    "total_rels = sum([stat['count'] for stat in rel_stats.get('query_result', [])])\n",
    "\n",
    "if total_nodes > 0 and total_rels > 0:\n",
    "    print(\"✅ SUCCESS! Knowledge graph construction completed!\")\n",
    "    print(f\"   📊 Total nodes: {total_nodes}\")\n",
    "    print(f\"   🔗 Total relationships: {total_rels}\")\n",
    "    print(\"\\n🔍 TO VISUALIZE IN NEO4J BROWSER:\")\n",
    "    print(\"   Run: CALL db.schema.visualization()\")\n",
    "    print(\"   Or: MATCH (n)-[r]->(m) RETURN n, r, m LIMIT 25\")\n",
    "else:\n",
    "    print(\"❌ Graph construction failed - no nodes or relationships created\")\n",
    "    \n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 29
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
